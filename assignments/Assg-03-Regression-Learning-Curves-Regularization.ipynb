{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 03: Regression, Learning Curves and Regularization\n",
    "\n",
    "**Due Date:** Friday 10/02/2020 (by 5pm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Please fill these in before submitting, just in case I accidentally mix up file names while grading**:\n",
    "\n",
    "Name: Joe Student\n",
    "\n",
    "CWID-5: (Last 5 digits of cwid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction \n",
    "\n",
    "In this exercise we will be using what you have learned about linear regression, polynomial regression and\n",
    "regularization, to explore an artificial dataset.\n",
    "\n",
    "I have generated a secret dataset.  The dataset uses a polynomial combination of a single parameter.\n",
    "The unknown function is no less than degree 3, but no more than a degree 15 polynomial.  And some random\n",
    "noise has also been added into the function, so that fitting it is not a completely trivial or obvious\n",
    "exercise.  Since the dataset is generated from a polynomial function, the output labels `y` are\n",
    "real valued numbers.  And thus you will be performing a regression fitting task in this assignment.\n",
    "\n",
    "Your task, should you choose to accept it, is to load and explore the data from this function.  Your ultimate\n",
    "goal is to try your best to determine the degree of the polynomial used, and the values of the parameters\n",
    "then used in the secret function.  Because of the noise added to the data you are given, you will not be able\n",
    "to exactly recover the parameters used to generate the artificial data.  You will even find that determining the\n",
    "exact degree of the generating polynomial function is not possible.  How you apply polynomial fitting and \n",
    "regularization techniques can give different and better or worse approximations of the true underlying function.\n",
    "\n",
    "In the below cells, I give instructions for the tasks you should attempt.  You will need to load the data and\n",
    "visualize it to begin with.  Then you will be asked to apply polynomial fitting and regularization in an attempt\n",
    "to fit the data.  But ultimately, at the end, you will be asked to take what you have discovered, and try and\n",
    "give your best answer for the polynomial degree and best fitted polynomial parameters for this unknown data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# By convention, we often just import the specific classes/functions\n",
    "# from scikit-learn we will need to train a model and perform prediction.\n",
    "# Here we include all of the classes and functions you should need for this\n",
    "# assignment from the sklearn library, but there could be other methods you might\n",
    "# want to try or would be useful to the way you approach the problem, so feel free\n",
    "# to import others you might need or want to try\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# notebook wide settings to make plots more readable and visually better to understand\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# notebook wide settings to make plots more readable and visually better to understand\n",
    "np.set_printoptions(suppress=True)\n",
    "plt.rc('axes', labelsize=14)\n",
    "plt.rc('xtick', labelsize=12)\n",
    "plt.rc('ytick', labelsize=12)\n",
    "plt.rc('figure', titlesize=18)\n",
    "plt.rc('legend', fontsize=14)\n",
    "plt.rcParams['figure.figsize'] = (12.0, 8.0) # default figure size if not specified in plot\n",
    "plt.style.use('seaborn-darkgrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Load Data, Explore and Visualize\n",
    "------------\n",
    "\n",
    "You have been given a set of 100 (artificial) data points in the file named `assg-03-data.csv` in our \n",
    "data subdirectory.  Start by loading this file into a pandas dataframe.  Explore the data a bit.  \n",
    "Use the `describe()` function to get a sense of the number of values (there should be `m=100` samples),\n",
    "and their mean and variance.  There are 2 columns, where `x` is the feature, and `y` is the function\n",
    "value, or in other words the label we will use for the regression fitting task.  What is the range of\n",
    "the `x` features?  What is the range of the `y` output label here?\n",
    "\n",
    "Also plot a scatter plot of the data to get a sense of the function shape.  Does it appear linear\n",
    "or nonlinear?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and explore the data.  Use describe and other functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# minimal data cleaning / preparation is needed, but if you load using pandas you will have to split out the x input column separate from the y output labels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the data we loaded using a simple scatter plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Create an Overfit Model\n",
    "\n",
    "You have been told the degree of the polynomial function governing the data you just loaded is somehwere\n",
    "between 3 and 15.  Lets start by overfitting a degree 20 polynomial regression to the data.\n",
    "In the next cells, use `PolynomialFeatures` and scikit-learn `LinearRegression()` to create a best fit\n",
    "degree 20 polynomial model of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# overfit with a degree 20 polynomial, display cross validation of fit\n",
    "# no regularization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell, use introspection of your fitted model to display the intercept and fitted coefficient parameters.\n",
    "Also discover the overall $R^2$ score of the fit.  Display them here for future reference.\n",
    "\n",
    "You should of course get a single intercept parameter, but 20 fitted theta coefficients.  Is there anything\n",
    "you think you can learn looking at the coefficients for this degree 20 fit?  For example, knowing that\n",
    "the true degree is less than 20 for the underlying function, do you have any guess at this point of what\n",
    "degree the polynomial might be that underlies this data?\n",
    "\n",
    "You should discover you get a pretty good $R^2$ score here, probably above 0.96.  This indicates a good\n",
    "fit that explains a lot of the variance seen.  But since we have a very high degree model, you should\n",
    "be worried at this point that at least some of that performance is coming from overfitting to the noise\n",
    "present in the data instead of the true function that underlies the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the intercept and coefficients of the fit as well as the R^2 score here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally for this part, visualize the fit from this degree 20 model.  Plot the raw data as a scatter\n",
    "plot again, and then use the `predict()` function to visualize the predictions made by the degree 20\n",
    "polynomial.\n",
    "\n",
    "Any insights from this visualization?  Do you see evidence of the type of extreme overfitting that we\n",
    "saw in the lectures? Especially around the ends of the data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the fit of the degree 20 polynomial here.  Start by plotting the raw data as a scatter plot\n",
    "\n",
    "\n",
    "# then display the fitted model using the predict() member function\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Cross Validation of Degree 20 Model\n",
    "------------------------------------------\n",
    "\n",
    "In these next parts of the assignment, we will walk you through applying regularization and using cross validation\n",
    "on your degree 20 model to try and discover a model that is not overfitting the noise\n",
    "present in the data set.  This will hopefully lead to better insights on the true nature of the\n",
    "function that may be generating the data you are analyzing.\n",
    "\n",
    "First of, for convenience, we recreate the `plot_learning_curves()` function from our textbook for\n",
    "our use in this part of the assignment.  Recall that this function, if you give it a \n",
    "scikit-learn `Pipeline()` model, and the `X` input data and `y` labels, will perform\n",
    "a series of cross validation trainings using the model and plot the results.  In this case, the\n",
    "function trains the model with a single input, then 2 inputs, and so on, and displays the\n",
    "resulting model predictions on the data it trained with, and on the held back validation data.\n",
    "As discussed in our lectures, these learning curves can help us determine whether a model is overfitting or\n",
    "underfitting, and what performance we can expect from a properly powerful trained and fitted model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curves(model, X, y):\n",
    "    \"\"\"Plot learning curves obtained with training the given scikit-learn model\n",
    "    with progressively larger amounts of the training data X.\n",
    "    \n",
    "    Nothing is returned explicitly from this function, but a plot will be created\n",
    "    and the resulting learning curves displayed on the plot.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model - A scikit-learn estimator model to be trained and evaluated.\n",
    "    X - The input training data\n",
    "    y - The target labels for training\n",
    "    \"\"\"\n",
    "    # we actually split out 20% of the data solely for validation, we train on the other 80%\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2)\n",
    "    \n",
    "    # keep track of history of the training and validation cost / error function\n",
    "    train_errors, val_errors = [], []\n",
    "    \n",
    "    # train on 1 to m of the data, up to all of the data in the split off training set\n",
    "    for m in range (1, len(X_train)):\n",
    "        # fit/train model on the first m samples of the data\n",
    "        model.fit(X_train[:m], y_train[:m])\n",
    "        \n",
    "        # get model predictions\n",
    "        y_train_predict = model.predict(X_train[:m])\n",
    "        y_val_predict = model.predict(X_val)\n",
    "        \n",
    "        # determine RMSE errors and save history for plotting\n",
    "        train_errors.append(mean_squared_error(y_train[:m], y_train_predict))\n",
    "        val_errors.append(mean_squared_error(y_val, y_val_predict))\n",
    "        \n",
    "    # plot the resulting learning curve\n",
    "    plt.plot(np.sqrt(train_errors), 'r-', linewidth=2, label='train')\n",
    "    plt.plot(np.sqrt(val_errors), 'b-', linewidth=2, label='val')\n",
    "    plt.xlabel('Training set size')\n",
    "    plt.ylabel('RMSE')\n",
    "    plt.legend(fontsize=18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the above function, display the learning curve for a degree 20 polynomial.  If you didn't do \n",
    "already, you will need a scikit-learn `Pipeline` here that first applies a `PolynomialFeatures` transformation\n",
    "to get a degree 20 set of input features, and then performs linear regression on the pipeline after\n",
    "creating the polynomial features.\n",
    "\n",
    "Use your pipeline transform/fit model to call the `plot_learning_curves()` function and display the\n",
    "learning curves for a degree 20 polynomial, that remember we should suspect is overfitting the\n",
    "data at this point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a pipeline here if needed for a degree 20 set of PolynomialFeatures that is then\n",
    "# trained with a standard LinearRegression\n",
    "\n",
    "# plot the learning curves.  You may need to change the limits of your plot, because if the data is overfitting\n",
    "# the performance on the validation data may be very bad compared to on the training data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can and will get different results when you plot the learning curve here because the train/validation\n",
    "split is done randomly each time.  So you should probably run the above plot of your learning curves\n",
    "more than 1 time, to get a feel for what results you can get.\n",
    "\n",
    "But you should observe 2 important points here\n",
    "\n",
    "1. Does it seem like the model is overfitting?  e.g. Are you observing that often there is a very big gap in\n",
    "performance between the validation and training RMSE measure, where validation RMSE is very bad compared to \n",
    "what is seen with the data model was trained with.\n",
    "2. You should make a note of what performance is reached on the training data RMSE measure with this overfit\n",
    "model.  This level of RMSE for training can probably be approached with a properly tuned model that\n",
    "generalizes well, and can thus get this performance on data it has not seen before.\n",
    "\n",
    "You might want to display the intercept, coefficients and $R^2$ score again here of the model you fit,\n",
    "just to confirm they are similiar to what you saw the first time.  But if you are using a pipeline, you\n",
    "may need to access these parameters in a slightly different way now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the intercept and coefficients of the fit as well as the R^2 score here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Applying Ridge Regularization\n",
    "---------------------\n",
    "\n",
    "In this section you will be asked to perform some of the regularization techniques we discussed in our\n",
    "lectures on linear regression and regularization.  The goal here is to try and get a better idea of what the\n",
    "true degree of the governing function might be, as well as the values of the coefficients of this function.\n",
    "\n",
    "Lets start by trying a simple \"ridge\" regularization.  Recall ridge regression applies $\\ell_2$ penalities\n",
    "(e.g. it adds in the square of the coefficient $\\theta_i^2$) which tends to reduce (but not eliminate)\n",
    "parameters that are not necessary for minimizing the fitness function.\n",
    "\n",
    "As before, create a pipeline that creates degree 20 polynomial features.  But apply and fit a ridge\n",
    "regression for this part of the assignment.  Try and explore alpha values to get a good model.\n",
    "A \"good\" model is one here that shows you are no longer overfitting.  You can tell you are no longer\n",
    "overfitting if there is no longer a big gap between training and validation performance.  Also you should compare\n",
    "the validation RMSE here to that achieved on the training data previously.  If your validation RMSE here \n",
    "is approaching that seen on only the training data before, the generalization of this fitted model\n",
    "with regularization is doing well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply l2 \"ridge\" regularization to a degree 20 set of polynomial features\n",
    "\n",
    "# plot the learning curves you observe for a good example of a ridge regularization fit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you think you have a relatively good alpha parameter for your ridge regression, display the\n",
    "intercept, coefficients and $R^2$ score of you fitted model here for future reference.  Compare the\n",
    "coefficients here to the previous overfitted model.  Do you have any insights on the degree of the\n",
    "underlying polynomial from looking at your coefficients here?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display intercept, coefficients and R^2 fit score here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Applying Lasso Regularization\n",
    "-----------------------\n",
    "\n",
    "In this part of the assignment we will next apply lasso regression. Recall that lasso regression is\n",
    "the same as $\\ell_1$ norm penality, which in practical terms means we use the absolute value of\n",
    "the coefficient in our regularization penalty term.\n",
    "\n",
    "As before, create a pipeline that creates degree 20 polynomial features.  But apply and fit a lasso\n",
    "regression for this part of the assignment.  You should explore `alpha` values around 0.001 to\n",
    "0.05 or so.  You may get warnings with this fit, try setting the tolerance parameter `tol` to \n",
    "0.1 here, and maybe increasing `max_iter` as well, though it is not nesessary to completely eliminate\n",
    "warnings to still get a relatively good fit here.  And actually you may want to explore higher values\n",
    "of `alpha`, as the higher you go, the more pressure on the fitted model to eliminate terms.  This may give\n",
    "you better insights into the true degree of the underlying polynomial function used to generate the\n",
    "data here.\n",
    "\n",
    "Once you have a good example, use your best fitted model to display the learning curves using \n",
    "the lasso regularization.\n",
    "\n",
    "Try and determine an `alpha` parameter that you think is working well here for the regularization.  You are\n",
    "doing \"well\" here if your learning curves indicate you are not ovrefitting, and you are approaching\n",
    "RMSE performance on your validation data somewhere around where training RMSE achieved in previous\n",
    "overfitting model.  And you are doing well if you maybe have some idea of the cutoff that looks likely for\n",
    "the true degree of the underlying function you are attempting to model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply l1 \"lasso\" regularization, to try and make unused terms drop out\n",
    "\n",
    "# plot the learning curves you observe for a good example of a lasso regularization fit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you think you have found relatively good `alpha` and other parameters, display the intercept, coefficients\n",
    "and $R^2$ scores again for this fitted model using lasso regularization.  Any observations about\n",
    "the coefficients now?  Compare them to the degree 20 model with no regularization.  You may be able to \n",
    "start getting some ideas on the true degree of the polynomial from the results here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display intercept, coefficients and R^2 fit score here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Give me Your Best Model Estimate\n",
    "\n",
    "Taking what you have observed from the previous parts 3-5 of the assignment, try and give you best guess/estimate\n",
    "for the true degree of the underlying polynomial.  Your lasso regularization results might be most useful\n",
    "for this determination, try finding values of `alpha` that are obviously too big (maybe by watching your\n",
    "$R^2$) score), and then reduce this a bit to get an estimate on the upper bound of the number of terms in\n",
    "the true polynomial.  Recall that because of noise you won't be able to get a perfect answer here.  Also it\n",
    "helps to know, especially for even powers of the polynomial, that coefficients here are often only\n",
    "reflecting lower even power effects.  So for example, for a true function with only a $x^2$ term, you might\n",
    "still get $x^4$ and $x^8$ coefficients using lasso regression, even with relatively high values of `alpha`.\n",
    "\n",
    "In any case, choose your best estimate of the \"true\" degree of the underlying polynomial function.  Then train\n",
    "a final linear regression with no or maybe slight $\\ell_2$ regularization to try and get a best estimate\n",
    "of the true model coefficients.  You should do a little bit of testing again using the learning curves\n",
    "and checking your $R^2$ fit score to determine that the model appears to be able to fit as well as\n",
    "your degree 20 models with regularization.  But then train a final model using all of the data, and report\n",
    "the intercept, coefficients and $R^2$ fit you achieve with your best estimated model for this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimate the polynomial degree and create your best model.  You might want to try first with no regularization,\n",
    "# and then maybe with a little bit of l-2 (ridge) regulariztion and compare.\n",
    "\n",
    "# you should confirm that you model does not overfit and performs as well as your degree 20 models with\n",
    "# regularization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display intercept, coefficients and R^2 fit score here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you settle on your best model, do a final trainin of it with all of the data.  Display\n",
    "your intercept, coefficients and $R^2$ fit score for this best model.\n",
    "\n",
    "Then, visualize the fit of your best model.  Once again scatter plot the raw data.  Then using the\n",
    "`predict()` function from you scikit-learn model, show the predicted regression as a line on top of the\n",
    "raw data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform one final fit of our best performing estimated model on all the data\n",
    "\n",
    "# fit on all the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display intercept, coefficients and R^2 fit score here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the fit, start with the raw data points we are fitting\n",
    "\n",
    "# display the best model  fit\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3-datasci",
   "language": "python",
   "name": "python3-datasci"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
